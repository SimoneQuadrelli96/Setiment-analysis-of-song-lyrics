{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T08:16:58.559591Z",
     "start_time": "2020-04-15T08:16:58.554640Z"
    }
   },
   "source": [
    "# Preprocessing\n",
    "This notebook executes preprocessing for both twitts and songs.\n",
    "Lemmatized and raw text adjectives and nouns are extracted from the texts (lyrics) and are saved along with additional information present in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T11:35:05.700279Z",
     "start_time": "2020-04-17T11:35:03.806555Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import patoolib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitts preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['anger-ratings-0to1.train.txt','fear-ratings-0to1.train.txt','joy-ratings-0to1.train.txt','sadness-ratings-0to1.train.txt']\n",
    "path = 'data/'\n",
    "data = []\n",
    "for file_name in file_names:\n",
    "    file_name = path+file_name\n",
    "    with open(file_name, encoding=\"utf8\") as chat:\n",
    "        chat_text = chat.read()\n",
    "        chat_text = chat_text[:len(chat_text)-1]\n",
    "    data += re.split(r'\\t+|\\n+', chat_text)\n",
    "data = np.array(data)\n",
    "data =np.reshape(data, (data.shape[0]//4, 4))\n",
    "df = pd.DataFrame(data=data, columns=[\"id\", \"text\",\"sentiment\",\"intensity\"])\n",
    "df.to_csv('data/training.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d430f780089433ba69ef133bbd9ac1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3613.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('data/training.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "docs, ids, sentiments, intensities= [], [], [], []\n",
    "data = tqdm_notebook(list(df.iterrows()))\n",
    "for i, row in data:\n",
    "    tokens = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    strs = tokenizer.tokenize(row['text'])\n",
    "    filtered_words = [word for word in strs if word not in stopwords.words('english')]\n",
    "    input_text =\" \".join(filtered_words)\n",
    "    for token in nlp(input_text):\n",
    "        tokens.append((token.text, token.lemma_, token.pos_))\n",
    "    docs.append(tokens)\n",
    "    ids.append([row['id']])\n",
    "    sentiments.append(row['sentiment'])\n",
    "    intensities.append(row['intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = []\n",
    "lemma = []\n",
    "for doc in docs:\n",
    "    sentence_raw = []\n",
    "    sentence_lemma = []\n",
    "    for elem in doc:\n",
    "        if elem[2] in ['NOUN','ADJ']:\n",
    "            sentence_raw.append(elem[0]) \n",
    "            sentence_lemma.append(elem[1])\n",
    "    raw.append(\" \".join(sentence_raw))\n",
    "    lemma.append(\" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['id'] =ids\n",
    "df['lemma'] = lemma\n",
    "df['raw'] = raw\n",
    "df['sentiment'] = sentiments\n",
    "df['intensity'] = intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>raw</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10000]</td>\n",
       "      <td>heck fridge landlord door angry</td>\n",
       "      <td>heck fridge landlord door angry</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10001]</td>\n",
       "      <td>driver word vehicle disgusted</td>\n",
       "      <td>driver word vehicle disgusted</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10002]</td>\n",
       "      <td>parcel pick store address fuming poorcustomers...</td>\n",
       "      <td>parcel pick store address fuming poorcustomers...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[10003]</td>\n",
       "      <td>butt wipe fire alarm asleep angry upset tired ...</td>\n",
       "      <td>butt wipe fire alarm asleep angry upset tired ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[10004]</td>\n",
       "      <td>phone talk rude money acc</td>\n",
       "      <td>phone talk rude money acc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>[40781]</td>\n",
       "      <td>home happy</td>\n",
       "      <td>home happy</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>[40782]</td>\n",
       "      <td>winter duvet</td>\n",
       "      <td>winter duvet</td>\n",
       "      <td>3</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>[40783]</td>\n",
       "      <td>sky background purple highlight dull color great</td>\n",
       "      <td>sky background purple highlights dull colors g...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>[40784]</td>\n",
       "      <td>second artist announcement good bluesfest2017 ...</td>\n",
       "      <td>second artist announcement good bluesfest2017 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>[40785]</td>\n",
       "      <td>creamy pesto chicken asparagus pine single day...</td>\n",
       "      <td>creamy pesto chicken asparagus pine single day...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              lemma  \\\n",
       "0     [10000]                    heck fridge landlord door angry   \n",
       "1     [10001]                      driver word vehicle disgusted   \n",
       "2     [10002]  parcel pick store address fuming poorcustomers...   \n",
       "3     [10003]  butt wipe fire alarm asleep angry upset tired ...   \n",
       "4     [10004]                          phone talk rude money acc   \n",
       "...       ...                                                ...   \n",
       "3608  [40781]                                         home happy   \n",
       "3609  [40782]                                       winter duvet   \n",
       "3610  [40783]   sky background purple highlight dull color great   \n",
       "3611  [40784]  second artist announcement good bluesfest2017 ...   \n",
       "3612  [40785]  creamy pesto chicken asparagus pine single day...   \n",
       "\n",
       "                                                    raw sentiment  intensity  \n",
       "0                       heck fridge landlord door angry         0      0.938  \n",
       "1                         driver word vehicle disgusted         0      0.896  \n",
       "2     parcel pick store address fuming poorcustomers...         0      0.896  \n",
       "3     butt wipe fire alarm asleep angry upset tired ...         0      0.896  \n",
       "4                             phone talk rude money acc         0      0.896  \n",
       "...                                                 ...       ...        ...  \n",
       "3608                                         home happy         3      0.104  \n",
       "3609                                       winter duvet         3      0.104  \n",
       "3610  sky background purple highlights dull colors g...         3      0.088  \n",
       "3611  second artist announcement good bluesfest2017 ...         3      0.083  \n",
       "3612  creamy pesto chicken asparagus pine single day...         3      0.083  \n",
       "\n",
       "[3613 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df['sentiment'] == 'anger'),'sentiment']=0\n",
    "df.loc[(df['sentiment'] == 'fear'),'sentiment']=1\n",
    "df.loc[(df['sentiment'] == 'joy'),'sentiment']=2\n",
    "df.loc[(df['sentiment'] == 'sadness'),'sentiment']=3\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/preprocessed_training.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitts preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T10:21:23.469022Z",
     "start_time": "2020-04-17T10:21:23.466030Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T11:35:02.392025Z",
     "start_time": "2020-04-17T11:35:01.406352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting data/lyrics.rar ...\n",
      "patool: running \"C:\\Program Files\\WinRAR\\rar.EXE\" x -- C:\\Users\\simone.quadrelli\\Desktop\\Setiment-analysis-of-song-lyrics\\data\\lyrics.rar\n",
      "patool:     with cwd=data/\n",
      "patool: ... data/lyrics.rar extracted to `data/'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patoolib.extract_archive(\"data/lyrics.rar\", outdir=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T11:35:31.459080Z",
     "start_time": "2020-04-17T11:35:09.003145Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lyrics.csv')\n",
    "df = df.dropna()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "docs, songs, genres, artists, ids = [],[],[],[],[]\n",
    "data = tqdm_notebook(list(df.iterrows()))\n",
    "for i, row in data:\n",
    "    tokens = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    strs = tokenizer.tokenize(row['lyrics'])\n",
    "    filtered_words = [word for word in strs if word not in stopwords.words('english')]\n",
    "    input_text =\" \".join(filtered_words)\n",
    "    for token in nlp(input_text):\n",
    "        tokens.append((token.text, token.lemma_, token.pos_))\n",
    "    docs.append(tokens)\n",
    "    ids.append([row['index']])\n",
    "    songs.append(row['song'])\n",
    "    genres.append(row['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T10:20:50.172428Z",
     "start_time": "2020-04-17T10:20:50.165448Z"
    }
   },
   "outputs": [],
   "source": [
    "raw = []\n",
    "lemma = []\n",
    "for doc in docs:\n",
    "    sentence_raw = []\n",
    "    sentence_lemma = []\n",
    "    for elem in doc:\n",
    "        if elem[2] in ['NOUN','ADJ']:\n",
    "            sentence_raw.append(elem[0]) \n",
    "            sentence_lemma.append(elem[1])\n",
    "    raw.append(\" \".join(sentence_raw))\n",
    "    lemma.append(\" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T10:20:50.775194Z",
     "start_time": "2020-04-17T10:20:50.761232Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['id'] =ids\n",
    "df['lemma'] = lemma\n",
    "df['raw'] = raw\n",
    "df['song'] = songs\n",
    "df['genre'] = genres\n",
    "df['author'] = df_all['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/preprocessed_songs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
